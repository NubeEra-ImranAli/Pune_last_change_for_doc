
# Ensemble: Deep Learning + XGBoost + SHAP Feature Importance

This branch contains an ensemble model that averages predictions from a trained deep learning model, SHAP Feature Importance and XGBoost regressor.

- ‚úÖ **Accuracy**: ~99% (R¬≤ Score)
- ‚ö°Ô∏è **Ensemble Method**: Soft Average
- ü§ñ Models:
  - Deep Neural Network (Keras)
  - XGBoost Regressor
  - SHAP feature importance
- üìà **Preprocessing**:
  - Polynomial Features (Degree 2)
  - StandardScaler
- üß™ Location-based test split for robustness
  
## ‚úÖ Final Model Comparison

| Model                                  | MAE     | RMSE    | R¬≤ Score | Remarks                                      |
|----------------------------------------|---------|---------|----------|----------------------------------------------|
| **SHAP Feature Importance (XGB)**      | 0.968   | 7.729   | 0.9928   | ‚ö° Best RMSE overall                          |
| **DL + XGB Stacking**                  | 1.006   | 7.970   | 0.9929   | ‚ö° Best R¬≤, strong ensemble                   |
| **Tuned XGBoost**                      | 0.944   | 13.040  | 0.9928   | Great MAE, slightly higher RMSE              |
| **Decision Tree Regression**           | 1.970   | 9.940   | 0.9888   | Good baseline tree model                     |
| **Random Forest Regression**           | 2.390   | 10.190  | 0.9882   | Slightly worse than decision tree            |
| **Linear Regression**                  | 32.560  | 43.080  | 0.7889   | ‚ùå Poor performance overall                   |


## File
- `shapModel.py`: Contains model definitions, ensemble logic, and performance metrics.

## Run
```bash
python shapModel.py
